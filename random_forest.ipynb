{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing_extensions import Self\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Node in a decision tree.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_col: str\n",
    "    ) -> None:\n",
    "        # For training\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.pk = self._set_pk()\n",
    "        self.gini = self._set_gini()\n",
    "\n",
    "        # For training/inference\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "        # For inference\n",
    "        self.feature = None\n",
    "        self.threshold = None\n",
    "\n",
    "    def _set_pk(self) -> float:\n",
    "        \"\"\"\n",
    "        Sets pk, the prop samples that are the positive class.\n",
    "        Assumes samples are an array of ints, where 1 is the positive\n",
    "        class and 0 is the negative class.\n",
    "        \"\"\"\n",
    "        return np.mean(self.df[self.target_col].values)\n",
    "\n",
    "    def _set_gini(self) -> float:\n",
    "        \"\"\"\n",
    "        Sets the Gini impurity.\n",
    "        \"\"\"\n",
    "        return 1 - self.pk**2 - (1 - self.pk)**2\n",
    "    \n",
    "    def split_on_feature(\n",
    "        self,\n",
    "        feature: str\n",
    "    ) -> tuple[float, int|float, Self, Self]:\n",
    "        \"\"\"\n",
    "        Iterate through values of a feature and identify split that\n",
    "        minimizes weighted Gini impurity in child nodes. Returns tuple\n",
    "        of weighted Gini impurity, feature threshold, and left and\n",
    "        right child nodes.\n",
    "        \"\"\"\n",
    "        values = []\n",
    "\n",
    "        for thresh in self.df[feature].unique():\n",
    "            values.append(self._process_split(feature, thresh))\n",
    "\n",
    "        values = [v for v in values if v[1] is not None]\n",
    "        if values:\n",
    "            return min(values, key=lambda x: x[0])\n",
    "        return None, None, None, None\n",
    "    \n",
    "    def _process_split(\n",
    "        self,\n",
    "        feature: str,\n",
    "        threshold: int|float\n",
    "    ) -> tuple[float, int|float, Self|None, Self|None]:\n",
    "        \"\"\"\n",
    "        Splits df on the feature threshold. Returns weighted Gini\n",
    "        impurity, inputted threshold, and child nodes. If split\n",
    "        results in empty subset, returns Gini impurity and None's.\n",
    "        \"\"\"\n",
    "        df_lower = self.df[self.df[feature] <= threshold]\n",
    "        df_upper = self.df[self.df[feature] > threshold]\n",
    "\n",
    "        # If threshold doesn't split the data at all, end early\n",
    "        if len(df_lower) == 0 or len(df_upper) == 0:\n",
    "            return self.gini, None, None, None\n",
    "\n",
    "        node_lower = Node(df_lower, self.target_col)\n",
    "        node_upper = Node(df_upper, self.target_col)\n",
    "\n",
    "        prop_lower = len(df_lower) / len(self.df)\n",
    "        prop_upper = len(df_upper) / len(self.df)\n",
    "\n",
    "        weighted_gini = node_lower.gini * prop_lower \\\n",
    "          + node_upper.gini * prop_upper\n",
    "\n",
    "        return weighted_gini, threshold, node_lower, node_upper\n",
    "    \n",
    "class DecisionTree:\n",
    "    \"\"\"\n",
    "    Decision tree classifier, constructed with Nodes\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target_col: str,\n",
    "        feature_select: float = 1.0,\n",
    "        max_depth: int = 4\n",
    "    ) -> None:\n",
    "        self.root = Node(df, target_col)\n",
    "        self.features = list(df)\n",
    "        self.features.remove(target_col)\n",
    "        self.feature_select = feature_select\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self) -> None:\n",
    "        \"\"\"\n",
    "        Builds tree using depth-first traversal\n",
    "        \"\"\"\n",
    "        stack = [(self.root, 0)]\n",
    "\n",
    "        while stack:\n",
    "            current_node, depth = stack.pop()\n",
    "\n",
    "            if depth <= self.max_depth:\n",
    "                left, right = self._process_node(current_node)\n",
    "\n",
    "                if left and right:\n",
    "                    current_node.left = left\n",
    "                    current_node.right = right\n",
    "                    stack.append((right, depth+1))\n",
    "                    stack.append((left, depth+1))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def _process_node(\n",
    "        self,\n",
    "        node: Node,\n",
    "        features: list[str]\n",
    "    ) -> tuple[Node|None, Node|None]:\n",
    "        \"\"\"\n",
    "        Iterates through features, identifies split that minimizes\n",
    "        Gini impurity in child nodes, and identifies feature whose\n",
    "        split minimizes Gini impurity the most. Then returns child\n",
    "        nodes split on that feature.\n",
    "        \"\"\"\n",
    "        # Randomly select features. No randomness if\n",
    "        # self.feature_select = 1.0 (default).\n",
    "        features = list(\n",
    "            np.random.choice(\n",
    "                self.features,\n",
    "                int(self.feature_select*len(self.features)),\n",
    "                replace=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Get Gini impurity for best split for each column\n",
    "        d = {}\n",
    "        for col in features:\n",
    "            feature_info = node.split_on_feature(col)\n",
    "            if feature_info[0] is not None:\n",
    "                d[col] = feature_info\n",
    "\n",
    "        # Select best column to split on\n",
    "        min_gini = np.inf\n",
    "        best_feature = None\n",
    "        for col, tup in d.items():\n",
    "            if tup[0] < min_gini:\n",
    "                min_gini = tup[0]\n",
    "                best_feature = col\n",
    "\n",
    "        # Only update if the best split reduces Gini impurity\n",
    "        if min_gini < node.gini:\n",
    "            # Update node\n",
    "            node.feature = best_feature\n",
    "            node.threshold = d[col][1]\n",
    "            return d[col][2:]\n",
    "\n",
    "        return None, None\n",
    "\n",
    "    \n",
    "    def classify(self, feature_df: pd.DataFrame) -> list[int]:\n",
    "        \"\"\"\n",
    "        Given a dataframe where each row is a feature vector, traverses\n",
    "        the tree to generate a predicted label.\n",
    "        \"\"\"\n",
    "        return [\n",
    "          self._classify(self.root, f) for i, f in feature_df.iterrows()\n",
    "        ]\n",
    "\n",
    "    def _classify(self, node: Node, features: pd.Series) -> int:\n",
    "        \"\"\"\n",
    "        Given a vector of features, traverse the node's children until\n",
    "        a leaf is reached, then return the most frequent class in the\n",
    "        node. If there are an equal number of positive and negative\n",
    "        labels, predicts the negative class.\n",
    "        \"\"\"\n",
    "        # Child node\n",
    "        if node.feature is None or node.threshold is None:\n",
    "            return int(node.pk > 0.5)\n",
    "\n",
    "        if features[node.feature] < node.threshold:\n",
    "            return self._classify(node.left, features)\n",
    "        return self._classify(node.right, features)\n",
    "\n",
    "    \n",
    "class RandomForest:\n",
    "    \"\"\"\n",
    "    Forest of decision trees.\n",
    "    \"\"\"\n",
    "    def __init__(self, df: pd.DataFrame, target_col: str, n_trees: int = 100, feature_select: float = 0.5, max_depth: int = 4) -> None:\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.n_trees = n_trees\n",
    "        self.feature_select = feature_select\n",
    "        self.max_depth = max_depth\n",
    "        self.forest = []\n",
    "        \n",
    "    def train(self) -> None:\n",
    "        \"\"\"\n",
    "        Fit the forest to self.df\n",
    "        \"\"\"\n",
    "        bootstrap_dfs = [self._bootstrap() for _ in range(self.n_trees)]\n",
    "        self.forest = [\n",
    "            DecisionTree(\n",
    "              bdf,\n",
    "              self.target_col,\n",
    "              self.feature_select,\n",
    "              self.max_depth\n",
    "            )\n",
    "            for bdf in bootstrap_dfs\n",
    "        ]\n",
    "        self.forest = [tree.build_tree() for tree in self.forest]\n",
    "        return None\n",
    "\n",
    "    def _bootstrap(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample rows from self.df with replacement\n",
    "        \"\"\"\n",
    "        return self.df.sample(len(self.df), replace=True)\n",
    "    \n",
    "    def classify(self, feature_df: pd.DataFrame) -> int:\n",
    "        \"\"\"\n",
    "        Classify inputted feature vectors. Each tree in the forest\n",
    "        generates a predicted label and the most common label for\n",
    "        each feature vector is returned.\n",
    "        \"\"\"\n",
    "        preds = pd.DataFrame(\n",
    "          [tree.classify(feature_df) for tree in self.forest]\n",
    "        )\n",
    "        # Return most common predicted label\n",
    "        return list(preds.mode().iloc[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating train and test data\n",
      "1. Fitting a decision tree\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DecisionTree._process_node() missing 1 required positional argument: 'features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1. Fitting a decision tree\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m decision_tree \u001b[38;5;241m=\u001b[39m DecisionTree(train_df, target_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mdecision_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m tree_preds \u001b[38;5;241m=\u001b[39m decision_tree\u001b[38;5;241m.\u001b[39mclassify(test_df)\n\u001b[1;32m     58\u001b[0m tree_accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(accuracy_score(test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], tree_preds), \u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[34], line 117\u001b[0m, in \u001b[0;36mDecisionTree.build_tree\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m current_node, depth \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m depth \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_depth:\n\u001b[0;32m--> 117\u001b[0m     left, right \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m left \u001b[38;5;129;01mand\u001b[39;00m right:\n\u001b[1;32m    120\u001b[0m         current_node\u001b[38;5;241m.\u001b[39mleft \u001b[38;5;241m=\u001b[39m left\n",
      "\u001b[0;31mTypeError\u001b[0m: DecisionTree._process_node() missing 1 required positional argument: 'features'"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Set seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "# Data generation helpers. Only parameter to pass in is the mean.\n",
    "npn = partial(np.random.normal, scale=1, size=1)\n",
    "npc = partial(np.random.choice, a=[0,1], size=1)\n",
    "\n",
    "# Use this function for a detailed look at decision tree formation\n",
    "def gen_df(n: int) -> pd.DataFrame:\n",
    "    labels = np.random.choice([0,1], n)\n",
    "    return pd.DataFrame({\n",
    "        'strong_continuous': [npn(3)[0] if x else npn(0)[0] for x in labels],\n",
    "        'weak_continuous': [npn(1)[0] if x else npn(0)[0] for x in labels],\n",
    "        'strong_categorical': [\n",
    "            npc(p=[0.8, 0.2])[0] if x else npc(p=[0.5,0.5])[0]\n",
    "            for x in labels\n",
    "        ],\n",
    "        'label': labels\n",
    "    })\n",
    "\n",
    "# Use this function for a random forest using many features\n",
    "def gen_df_hd(n_rows: int, n_cols: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Note: actual df has 2*n_cols, since we produce both\n",
    "    a continuous and categorical feature for each col.\n",
    "    \"\"\"\n",
    "\n",
    "    labels = np.random.choice([0,1], n_rows)\n",
    "    d = {'label': labels}\n",
    "\n",
    "    for i in range(n_cols):\n",
    "        mean = np.random.uniform(0, 1, 1)[0]\n",
    "        d[f\"continuous_{i}\"] = [\n",
    "            npn(mean)[0] if x else npn(0)[0] for x in labels\n",
    "        ]\n",
    "        ratio = np.random.uniform(0.4, 0.6, 1)[0]\n",
    "        d[f\"categorical_{i}\"] = [\n",
    "            npc(p=[ratio, 1-ratio])[0] if x else npc(p=[0.5,0.5])[0]\n",
    "            for x in labels\n",
    "        ]\n",
    "\n",
    "    return pd.DataFrame(d)\n",
    "\n",
    "# Generate data\n",
    "print(\"Generating train and test data\")\n",
    "train_df = gen_df_hd(400, 50)\n",
    "test_df = gen_df_hd(100, 50)\n",
    "\n",
    "# 1. Decision Tree\n",
    "print(\"1. Fitting a decision tree\")\n",
    "decision_tree = DecisionTree(train_df, target_col='label')\n",
    "decision_tree.build_tree()\n",
    "tree_preds = decision_tree.classify(test_df)\n",
    "tree_accuracy = round(accuracy_score(test_df['label'], tree_preds), 3)\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"2. Fitting a random forest\")\n",
    "forest = RandomForest(train_df, target_col='label', n_trees=50)\n",
    "forest.train()\n",
    "forest_preds = forest.classify(test_df)\n",
    "forest_accuracy = round(accuracy_score(test_df['label'], forest_preds), 3)\n",
    "\n",
    "# 3. Average tree in forest\n",
    "print(\"3. Calculating average tree accuracy\")\n",
    "tree_accs = []\n",
    "for i in range(forest.n_trees):\n",
    "    forest_tree_preds = forest.forest[i].classify(test_df)\n",
    "    tree_accs.append(accuracy_score(test_df['label'], forest_tree_preds))\n",
    "forest_tree_accuracy = np.mean(tree_accs).round(3)\n",
    "\n",
    "# 4. Scikit-learn\n",
    "print(\"4. Fitting scikit-learn forest\")\n",
    "X_train = train_df.copy()\n",
    "X_train = X_train.drop('label', axis=1)\n",
    "\n",
    "X_test = test_df.copy()\n",
    "X_test = X_test.drop('label', axis=1)\n",
    "\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "sklearn_preds = rf.predict(X_test)\n",
    "sklearn_acc = round(accuracy_score(y_test, sklearn_preds), 3)\n",
    "\n",
    "# Display results\n",
    "print(\"Accuracy\")\n",
    "print(f\" * Single decision tree:   {tree_accuracy}\")\n",
    "print(f\" * Avg random forest tree: {forest_tree_accuracy}\")\n",
    "print(f\" * Full random forest:     {forest_accuracy}\")\n",
    "print(f\" * Scikit-learn forest:    {sklearn_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
